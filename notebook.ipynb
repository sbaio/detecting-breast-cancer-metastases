{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd056edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33",
   "display_name": "Python 3.8.8 64-bit ('sbaio': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "56edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tensorboardX import SummaryWriter\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import copy\n",
    "\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "train_tile_annotations = pd.read_csv(data_dir / \"train_input/train_tile_annotations.csv\")\n",
    "\n",
    "def get_features(path, ntiles=1000):\n",
    "    x = np.load(path)[:,3:]\n",
    "\n",
    "    y = np.tile(x,(ntiles//x.shape[0],1))\n",
    "    if y.shape[0] < ntiles:\n",
    "        ncat = ntiles%x.shape[0]\n",
    "        y = np.concatenate([y, x[:ncat]], axis=0)\n",
    "    resnet_features = y # of size 1000 x 2048\n",
    "    return resnet_features\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.training_output = pd.read_csv(data_dir / \"training_output.csv\")\n",
    "        self.ntiles = 1000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.training_output.iloc[i]\n",
    "        ID, target = x['ID'], x['Target']\n",
    "        \n",
    "        # load the pre-computed resnet features\n",
    "        feat_path = glob(f\"data/train_input/resnet_features/ID_{ID:03d}*.npy\")[0]\n",
    "        x = get_features(feat_path, ntiles=self.ntiles)\n",
    "        resnet_features = torch.from_numpy(x).float()\n",
    "        \n",
    "        return resnet_features, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_output)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        fSz = 2048 # dimension of resnet features\n",
    "        self.conv1x1 = nn.Conv1d(fSz, 1, kernel_size=1, bias=False)\n",
    "\n",
    "        self.R = 5\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.R, 200),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "        # TODO: Replace Sigmoid with ReLU\n",
    "\n",
    "        self.BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, feats, targets=None):\n",
    "        \"\"\"\n",
    "            feats: torch.FloatTensor features of the tiles of size bSz,ntiles, fSz \n",
    "        \"\"\"\n",
    "        \n",
    "        # bSz, ntiles, fSz\n",
    "        feats = feats.transpose(1,2)      # Adapt input for conv\n",
    "        # bSz, fSz, ntiles\n",
    "        feats = self.conv1x1(feats)[:,0]  # Feature embedding\n",
    "        # bSz, ntiles\n",
    "\n",
    "        # min-max selection\n",
    "        vals, inds = feats.sort(dim=1)\n",
    "        minmax_inds = torch.cat([inds[:,:self.R] , inds[:,-self.R:]], dim=1)\n",
    "        minmax_feats = torch.gather(feats, dim=1, index=minmax_inds)\n",
    "\n",
    "        logits = self.mlp(minmax_feats)\n",
    "        # bSz, 1\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        out = {\n",
    "            'logits':logits, 'probs':probs,\n",
    "        }\n",
    "        if targets is not None:\n",
    "            # compute loss\n",
    "            loss = self.BCELoss(logits, targets[:,None].float())\n",
    "            loss += 0.5 * (self.conv1x1.weight**2).sum() # Add weight decay\n",
    "            out['loss'] = loss\n",
    "            \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset()\n",
    "model_ = Model()\n",
    "\n",
    "\n",
    "# # TODO: data augmentation ?\n",
    "run_i = 0\n",
    "\n",
    "def fit_and_score(train_dset, val_dset):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dset, batch_size=10, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dset, batch_size=10, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = copy.deepcopy(model_).cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train loop\n",
    "    nepochs = 30\n",
    "\n",
    "    def validate():\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for feats, targets in val_loader:\n",
    "            all_targets.append(targets.numpy())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(feats.cuda())\n",
    "            all_preds.append(out['probs'].cpu().numpy())\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        auc = sklearn.metrics.roc_auc_score(all_targets, all_preds)\n",
    "\n",
    "        return auc\n",
    "\n",
    "    global run_i\n",
    "    writer = SummaryWriter(f'runs/run_{run_i}')\n",
    "    run_i += 1\n",
    "\n",
    "    iteration = 0\n",
    "    best_auc = 0\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        for feats, targets in train_loader:\n",
    "            feats = feats.cuda()\n",
    "            targets = targets.cuda()\n",
    "            out = model(feats, targets=targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out['loss'].backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Train/loss\", out['loss'], iteration)\n",
    "            iteration += 1\n",
    "\n",
    "        # validate and keep the best at each epoch\n",
    "        auc = validate()\n",
    "        writer.add_scalar(\"Val/AUC\", auc, iteration)\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "        print(f\"Epoch {epoch}: AUC {auc:0.2f}, best AUC {best_auc:0.2f}\")\n",
    "    \n",
    "\n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: AUC 0.63, best AUC 0.63\n",
      "Epoch 1: AUC 0.46, best AUC 0.63\n",
      "Epoch 2: AUC 0.53, best AUC 0.63\n",
      "Epoch 3: AUC 0.63, best AUC 0.63\n",
      "Epoch 4: AUC 0.51, best AUC 0.63\n",
      "Epoch 5: AUC 0.47, best AUC 0.63\n",
      "Epoch 6: AUC 0.63, best AUC 0.63\n",
      "Epoch 7: AUC 0.57, best AUC 0.63\n",
      "Epoch 8: AUC 0.56, best AUC 0.63\n",
      "Epoch 9: AUC 0.54, best AUC 0.63\n",
      "Epoch 10: AUC 0.53, best AUC 0.63\n",
      "Epoch 11: AUC 0.51, best AUC 0.63\n",
      "Epoch 12: AUC 0.55, best AUC 0.63\n",
      "Epoch 13: AUC 0.67, best AUC 0.67\n",
      "Epoch 14: AUC 0.82, best AUC 0.82\n",
      "Epoch 15: AUC 0.81, best AUC 0.82\n",
      "Epoch 16: AUC 0.75, best AUC 0.82\n",
      "Epoch 17: AUC 0.90, best AUC 0.90\n",
      "Epoch 18: AUC 0.77, best AUC 0.90\n",
      "Epoch 19: AUC 0.74, best AUC 0.90\n",
      "Epoch 20: AUC 0.86, best AUC 0.90\n",
      "Epoch 21: AUC 0.87, best AUC 0.90\n",
      "Epoch 22: AUC 0.82, best AUC 0.90\n",
      "Epoch 23: AUC 0.86, best AUC 0.90\n",
      "Epoch 24: AUC 0.86, best AUC 0.90\n",
      "Epoch 25: AUC 0.87, best AUC 0.90\n",
      "Epoch 26: AUC 0.86, best AUC 0.90\n",
      "Epoch 27: AUC 0.88, best AUC 0.90\n",
      "Epoch 28: AUC 0.89, best AUC 0.90\n",
      "Epoch 29: AUC 0.89, best AUC 0.90\n",
      "CV [0/5] -  AUC: 0.90\n",
      "Epoch 0: AUC 0.61, best AUC 0.61\n",
      "Epoch 1: AUC 0.54, best AUC 0.61\n",
      "Epoch 2: AUC 0.41, best AUC 0.61\n",
      "Epoch 3: AUC 0.49, best AUC 0.61\n",
      "Epoch 4: AUC 0.55, best AUC 0.61\n",
      "Epoch 5: AUC 0.65, best AUC 0.65\n",
      "Epoch 6: AUC 0.59, best AUC 0.65\n",
      "Epoch 7: AUC 0.67, best AUC 0.67\n",
      "Epoch 8: AUC 0.71, best AUC 0.71\n",
      "Epoch 9: AUC 0.60, best AUC 0.71\n",
      "Epoch 10: AUC 0.60, best AUC 0.71\n",
      "Epoch 11: AUC 0.59, best AUC 0.71\n",
      "Epoch 12: AUC 0.55, best AUC 0.71\n",
      "Epoch 13: AUC 0.57, best AUC 0.71\n",
      "Epoch 14: AUC 0.54, best AUC 0.71\n",
      "Epoch 15: AUC 0.49, best AUC 0.71\n",
      "Epoch 16: AUC 0.56, best AUC 0.71\n",
      "Epoch 17: AUC 0.51, best AUC 0.71\n",
      "Epoch 18: AUC 0.44, best AUC 0.71\n",
      "Epoch 19: AUC 0.56, best AUC 0.71\n",
      "Epoch 20: AUC 0.56, best AUC 0.71\n",
      "Epoch 21: AUC 0.55, best AUC 0.71\n",
      "Epoch 22: AUC 0.60, best AUC 0.71\n",
      "Epoch 23: AUC 0.66, best AUC 0.71\n",
      "Epoch 24: AUC 0.52, best AUC 0.71\n",
      "Epoch 25: AUC 0.48, best AUC 0.71\n",
      "Epoch 26: AUC 0.50, best AUC 0.71\n",
      "Epoch 27: AUC 0.52, best AUC 0.71\n",
      "Epoch 28: AUC 0.44, best AUC 0.71\n",
      "Epoch 29: AUC 0.43, best AUC 0.71\n",
      "CV [1/5] -  AUC: 0.71\n",
      "Epoch 0: AUC 0.47, best AUC 0.47\n",
      "Epoch 1: AUC 0.35, best AUC 0.47\n",
      "Epoch 2: AUC 0.43, best AUC 0.47\n",
      "Epoch 3: AUC 0.43, best AUC 0.47\n",
      "Epoch 4: AUC 0.50, best AUC 0.50\n",
      "Epoch 5: AUC 0.57, best AUC 0.57\n",
      "Epoch 6: AUC 0.42, best AUC 0.57\n",
      "Epoch 7: AUC 0.49, best AUC 0.57\n",
      "Epoch 8: AUC 0.47, best AUC 0.57\n",
      "Epoch 9: AUC 0.46, best AUC 0.57\n",
      "Epoch 10: AUC 0.52, best AUC 0.57\n",
      "Epoch 11: AUC 0.42, best AUC 0.57\n",
      "Epoch 12: AUC 0.58, best AUC 0.58\n",
      "Epoch 13: AUC 0.59, best AUC 0.59\n",
      "Epoch 14: AUC 0.50, best AUC 0.59\n",
      "Epoch 15: AUC 0.32, best AUC 0.59\n",
      "Epoch 16: AUC 0.52, best AUC 0.59\n",
      "Epoch 17: AUC 0.48, best AUC 0.59\n",
      "Epoch 18: AUC 0.50, best AUC 0.59\n",
      "Epoch 19: AUC 0.54, best AUC 0.59\n",
      "Epoch 20: AUC 0.46, best AUC 0.59\n",
      "Epoch 21: AUC 0.56, best AUC 0.59\n",
      "Epoch 22: AUC 0.69, best AUC 0.69\n",
      "Epoch 23: AUC 0.84, best AUC 0.84\n",
      "Epoch 24: AUC 0.83, best AUC 0.84\n",
      "Epoch 25: AUC 0.81, best AUC 0.84\n",
      "Epoch 26: AUC 0.89, best AUC 0.89\n",
      "Epoch 27: AUC 0.87, best AUC 0.89\n",
      "Epoch 28: AUC 0.84, best AUC 0.89\n",
      "Epoch 29: AUC 0.82, best AUC 0.89\n",
      "CV [2/5] -  AUC: 0.89\n",
      "Epoch 0: AUC 0.65, best AUC 0.65\n",
      "Epoch 1: AUC 0.49, best AUC 0.65\n",
      "Epoch 2: AUC 0.62, best AUC 0.65\n",
      "Epoch 3: AUC 0.48, best AUC 0.65\n",
      "Epoch 4: AUC 0.68, best AUC 0.68\n",
      "Epoch 5: AUC 0.60, best AUC 0.68\n",
      "Epoch 6: AUC 0.55, best AUC 0.68\n",
      "Epoch 7: AUC 0.51, best AUC 0.68\n",
      "Epoch 8: AUC 0.66, best AUC 0.68\n",
      "Epoch 9: AUC 0.65, best AUC 0.68\n",
      "Epoch 10: AUC 0.49, best AUC 0.68\n",
      "Epoch 11: AUC 0.62, best AUC 0.68\n",
      "Epoch 12: AUC 0.55, best AUC 0.68\n",
      "Epoch 13: AUC 0.70, best AUC 0.70\n",
      "Epoch 14: AUC 0.41, best AUC 0.70\n",
      "Epoch 15: AUC 0.43, best AUC 0.70\n",
      "Epoch 16: AUC 0.70, best AUC 0.70\n",
      "Epoch 17: AUC 0.58, best AUC 0.70\n",
      "Epoch 18: AUC 0.61, best AUC 0.70\n",
      "Epoch 19: AUC 0.58, best AUC 0.70\n",
      "Epoch 20: AUC 0.52, best AUC 0.70\n",
      "Epoch 21: AUC 0.68, best AUC 0.70\n",
      "Epoch 22: AUC 0.63, best AUC 0.70\n",
      "Epoch 23: AUC 0.54, best AUC 0.70\n",
      "Epoch 24: AUC 0.52, best AUC 0.70\n",
      "Epoch 25: AUC 0.40, best AUC 0.70\n",
      "Epoch 26: AUC 0.55, best AUC 0.70\n",
      "Epoch 27: AUC 0.51, best AUC 0.70\n",
      "Epoch 28: AUC 0.48, best AUC 0.70\n",
      "Epoch 29: AUC 0.56, best AUC 0.70\n",
      "CV [3/5] -  AUC: 0.70\n",
      "Epoch 0: AUC 0.47, best AUC 0.47\n",
      "Epoch 1: AUC 0.40, best AUC 0.47\n",
      "Epoch 2: AUC 0.56, best AUC 0.56\n",
      "Epoch 3: AUC 0.64, best AUC 0.64\n",
      "Epoch 4: AUC 0.50, best AUC 0.64\n",
      "Epoch 5: AUC 0.58, best AUC 0.64\n",
      "Epoch 6: AUC 0.54, best AUC 0.64\n",
      "Epoch 7: AUC 0.56, best AUC 0.64\n",
      "Epoch 8: AUC 0.59, best AUC 0.64\n",
      "Epoch 9: AUC 0.49, best AUC 0.64\n",
      "Epoch 10: AUC 0.59, best AUC 0.64\n",
      "Epoch 11: AUC 0.55, best AUC 0.64\n",
      "Epoch 12: AUC 0.58, best AUC 0.64\n",
      "Epoch 13: AUC 0.47, best AUC 0.64\n",
      "Epoch 14: AUC 0.55, best AUC 0.64\n",
      "Epoch 15: AUC 0.65, best AUC 0.65\n",
      "Epoch 16: AUC 0.55, best AUC 0.65\n",
      "Epoch 17: AUC 0.48, best AUC 0.65\n",
      "Epoch 18: AUC 0.58, best AUC 0.65\n",
      "Epoch 19: AUC 0.53, best AUC 0.65\n",
      "Epoch 20: AUC 0.49, best AUC 0.65\n",
      "Epoch 21: AUC 0.51, best AUC 0.65\n",
      "Epoch 22: AUC 0.48, best AUC 0.65\n",
      "Epoch 23: AUC 0.49, best AUC 0.65\n",
      "Epoch 24: AUC 0.59, best AUC 0.65\n",
      "Epoch 25: AUC 0.62, best AUC 0.65\n",
      "Epoch 26: AUC 0.67, best AUC 0.67\n",
      "Epoch 27: AUC 0.52, best AUC 0.67\n",
      "Epoch 28: AUC 0.54, best AUC 0.67\n",
      "Epoch 29: AUC 0.52, best AUC 0.67\n",
      "CV [4/5] -  AUC: 0.67\n",
      "Predicting weak labels using Chowder\n",
      "AUC: mean 0.7746739658853121, std 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_runs = 1\n",
    "num_splits = 5\n",
    "\n",
    "aucs = []\n",
    "for seed in range(num_runs):\n",
    "    # create new model\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True,random_state=seed)\n",
    "\n",
    "    targets = dset.training_output['Target'].tolist()\n",
    "\n",
    "    cv_aucs = []\n",
    "    for i, (train_inds, val_inds) in enumerate(cv.split(dset, y=targets)):\n",
    "        train_dset = torch.utils.data.Subset(dset, train_inds)\n",
    "        val_dset = torch.utils.data.Subset(dset, val_inds)\n",
    "\n",
    "        # train model on train_dset; evaluate on val_dset\n",
    "\n",
    "        auc = fit_and_score(train_dset, val_dset)\n",
    "        print(f\"CV [{i}/{num_splits}] -  AUC: {auc:0.2f}\")\n",
    "\n",
    "        cv_aucs.append(auc)\n",
    "\n",
    "    cv_auc = np.array(cv_aucs).mean()\n",
    "    aucs.append(cv_auc)\n",
    "\n",
    "aucs = np.array(aucs)\n",
    "\n",
    "print(\"Predicting weak labels using Chowder\")\n",
    "print(\"AUC: mean {}, std {}\".format(aucs.mean(), aucs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}