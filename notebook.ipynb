{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd056edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33",
   "display_name": "Python 3.8.8 64-bit ('sbaio': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "56edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tensorboardX import SummaryWriter\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import copy\n",
    "\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "train_tile_annotations = pd.read_csv(data_dir / \"train_input/train_tile_annotations.csv\")\n",
    "\n",
    "def get_features(path, ntiles=1000):\n",
    "    x = np.load(path)[:,3:]\n",
    "\n",
    "    y = np.tile(x,(ntiles//x.shape[0],1))\n",
    "    if y.shape[0] < ntiles:\n",
    "        ncat = ntiles%x.shape[0]\n",
    "        y = np.concatenate([y, x[:ncat]], axis=0)\n",
    "    resnet_features = y # of size 1000 x 2048\n",
    "    return resnet_features\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.training_output = pd.read_csv(data_dir / \"training_output.csv\")\n",
    "        self.ntiles = 1000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.training_output.iloc[i]\n",
    "        ID, target = x['ID'], x['Target']\n",
    "        \n",
    "        # load the pre-computed resnet features\n",
    "        feat_path = glob(f\"data/train_input/resnet_features/ID_{ID:03d}*.npy\")[0]\n",
    "        x = get_features(feat_path, ntiles=self.ntiles)\n",
    "        resnet_features = torch.from_numpy(x).float()\n",
    "        \n",
    "        return resnet_features, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_output)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        fSz = 2048 # dimension of resnet features\n",
    "        self.conv1x1 = nn.Conv1d(fSz, 1, kernel_size=1, bias=False)\n",
    "\n",
    "        self.R = 5\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.R, 200),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "        # TODO: Replace Sigmoid with ReLU\n",
    "\n",
    "        self.BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, feats, targets=None):\n",
    "        \"\"\"\n",
    "            feats: torch.FloatTensor features of the tiles of size bSz,ntiles, fSz \n",
    "        \"\"\"\n",
    "        \n",
    "        # bSz, ntiles, fSz\n",
    "        feats = feats.transpose(1,2)      # Adapt input for conv\n",
    "        # bSz, fSz, ntiles\n",
    "        feats = self.conv1x1(feats)[:,0]  # Feature embedding\n",
    "        # bSz, ntiles\n",
    "\n",
    "        # min-max selection\n",
    "        vals, inds = feats.sort(dim=1)\n",
    "        minmax_inds = torch.cat([inds[:,:self.R] , inds[:,-self.R:]], dim=1)\n",
    "        minmax_feats = torch.gather(feats, dim=1, index=minmax_inds)\n",
    "\n",
    "        logits = self.mlp(minmax_feats)\n",
    "        # bSz, 1\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        out = {\n",
    "            'logits':logits, 'probs':probs,\n",
    "        }\n",
    "        if targets is not None:\n",
    "            # compute loss\n",
    "            loss = self.BCELoss(logits, targets[:,None].float())\n",
    "            loss += 0.5 * (self.conv1x1.weight**2).sum() # Add weight decay\n",
    "            out['loss'] = loss\n",
    "            \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset()\n",
    "model_ = Model()\n",
    "\n",
    "\n",
    "# # TODO: data augmentation ?\n",
    "run_i = 0\n",
    "\n",
    "def fit_and_score(train_dset, val_dset):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dset, batch_size=10, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dset, batch_size=10, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = copy.deepcopy(model_).cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train loop\n",
    "    nepochs = 30\n",
    "\n",
    "    def validate():\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for feats, targets in val_loader:\n",
    "            all_targets.append(targets.numpy())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(feats.cuda())\n",
    "            all_preds.append(out['probs'].cpu().numpy())\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        auc = sklearn.metrics.roc_auc_score(all_targets, all_preds)\n",
    "\n",
    "        return auc\n",
    "\n",
    "    global run_i\n",
    "    writer = SummaryWriter(f'runs/run_{run_i}')\n",
    "    run_i += 1\n",
    "\n",
    "    iteration = 0\n",
    "    best_auc = 0\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        for feats, targets in train_loader:\n",
    "            feats = feats.cuda()\n",
    "            targets = targets.cuda()\n",
    "            out = model(feats, targets=targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out['loss'].backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Train/loss\", out['loss'], iteration)\n",
    "            iteration += 1\n",
    "\n",
    "        # validate and keep the best at each epoch\n",
    "        auc = validate()\n",
    "        writer.add_scalar(\"Val/AUC\", auc, iteration)\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "        print(f\"Epoch {epoch}: AUC {auc:0.2f}, best AUC {best_auc:0.2f}\")\n",
    "    \n",
    "\n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: AUC 0.5240641711229946, best AUC 0.5240641711229946\n",
      "Epoch 1: AUC 0.5655080213903744, best AUC 0.5655080213903744\n",
      "Epoch 2: AUC 0.5053475935828877, best AUC 0.5655080213903744\n",
      "Epoch 3: AUC 0.5574866310160427, best AUC 0.5655080213903744\n",
      "Epoch 4: AUC 0.5762032085561497, best AUC 0.5762032085561497\n",
      "Epoch 5: AUC 0.4358288770053476, best AUC 0.5762032085561497\n",
      "Epoch 6: AUC 0.5106951871657754, best AUC 0.5762032085561497\n",
      "Epoch 7: AUC 0.5280748663101604, best AUC 0.5762032085561497\n",
      "Epoch 8: AUC 0.6323529411764706, best AUC 0.6323529411764706\n",
      "Epoch 9: AUC 0.5120320855614973, best AUC 0.6323529411764706\n",
      "Epoch 10: AUC 0.5521390374331551, best AUC 0.6323529411764706\n",
      "Epoch 11: AUC 0.7954545454545454, best AUC 0.7954545454545454\n",
      "Epoch 12: AUC 0.7593582887700534, best AUC 0.7954545454545454\n",
      "Epoch 13: AUC 0.9064171122994652, best AUC 0.9064171122994652\n",
      "Epoch 14: AUC 0.9037433155080213, best AUC 0.9064171122994652\n",
      "Epoch 15: AUC 0.8061497326203209, best AUC 0.9064171122994652\n",
      "Epoch 16: AUC 0.8368983957219251, best AUC 0.9064171122994652\n",
      "Epoch 17: AUC 0.8221925133689839, best AUC 0.9064171122994652\n",
      "Epoch 18: AUC 0.7914438502673796, best AUC 0.9064171122994652\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e6952bd315ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# train model on train_dset; evaluate on val_dset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CV [{i}/{num_splits}] -  AUC: {auc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-02ea3f53c9d7>\u001b[0m in \u001b[0;36mfit_and_score\u001b[0;34m(train_dset, val_dset)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_runs = 1\n",
    "num_splits = 5\n",
    "\n",
    "aucs = []\n",
    "for seed in range(num_runs):\n",
    "    # create new model\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True,random_state=seed)\n",
    "\n",
    "    targets = dset.training_output['Target'].tolist()\n",
    "\n",
    "    cv_aucs = []\n",
    "    for i, (train_inds, val_inds) in enumerate(cv.split(dset, y=targets)):\n",
    "        train_dset = torch.utils.data.Subset(dset, train_inds)\n",
    "        val_dset = torch.utils.data.Subset(dset, val_inds)\n",
    "\n",
    "        # train model on train_dset; evaluate on val_dset\n",
    "\n",
    "        auc = fit_and_score(train_dset, val_dset)\n",
    "        print(f\"CV [{i}/{num_splits}] -  AUC: {auc:0.2f}\")\n",
    "\n",
    "        cv_aucs.append(auc)\n",
    "\n",
    "    cv_auc = np.array(cv_aucs).mean()\n",
    "    aucs.append(cv_auc)\n",
    "\n",
    "aucs = np.array(aucs)\n",
    "\n",
    "print(\"Predicting weak labels using Chowder\")\n",
    "print(\"AUC: mean {}, std {}\".format(aucs.mean(), aucs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}