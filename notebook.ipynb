{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd056edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33",
   "display_name": "Python 3.8.8 64-bit ('sbaio': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "56edcafb40a48ac9331f4cd83ce0dae2cb35b505e0a45e3fd20930a2ac883a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tensorboardX import SummaryWriter\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import copy\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "train_tile_annotations = pd.read_csv(data_dir / \"train_input/train_tile_annotations.csv\")\n",
    "\n",
    "def get_features(path, ntiles=1000):\n",
    "    x = np.load(path)[:,3:]\n",
    "    # tile features to have 1000 ones\n",
    "    y = np.tile(x,(ntiles//x.shape[0],1))\n",
    "    if y.shape[0] < ntiles:\n",
    "        ncat = ntiles%x.shape[0]\n",
    "        y = np.concatenate([y, x[:ncat]], axis=0)\n",
    "    resnet_features = y # of size 1000 x 2048\n",
    "    return resnet_features\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.training_output = pd.read_csv(data_dir / \"training_output.csv\")\n",
    "        self.ntiles = 1000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.training_output.iloc[i]\n",
    "        ID, target = x['ID'], x['Target']\n",
    "        \n",
    "        # load the pre-computed resnet features\n",
    "        feat_path = glob(f\"data/train_input/resnet_features/ID_{ID:03d}*.npy\")[0]\n",
    "        x = get_features(feat_path, ntiles=self.ntiles)\n",
    "        resnet_features = torch.from_numpy(x).float()\n",
    "        \n",
    "        return resnet_features, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_output)\n",
    "\n",
    "class TestDataset():\n",
    "    def __init__(self):\n",
    "        self.test_features_paths = sorted(glob(\"data/test_input/resnet_features/ID_*.npy\"))\n",
    "        print(f\"Test dataset has {len(self.test_features_paths)}\")\n",
    "        self.ntiles=  1000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        feat_path = self.test_features_paths[i]\n",
    "        ID = feat_path.split('/')[-1].split('.')[0].replace('ID_','')\n",
    "        # load the pre-computed resnet features\n",
    "        x = get_features(feat_path, ntiles=self.ntiles)\n",
    "        resnet_features = torch.from_numpy(x).float()\n",
    "        \n",
    "        return resnet_features, ID\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_features_paths)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        fSz = 2048 # dimension of resnet features\n",
    "        self.conv1x1 = nn.Conv1d(fSz, 1, kernel_size=1, bias=False)\n",
    "\n",
    "        self.R = 5\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.R, 200),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "        # TODO: Replace Sigmoid with ReLU\n",
    "\n",
    "        self.BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, feats, targets=None):\n",
    "        \"\"\"\n",
    "            feats: torch.FloatTensor features of the tiles of size bSz,ntiles, fSz \n",
    "        \"\"\"\n",
    "        \n",
    "        # bSz, ntiles, fSz\n",
    "        feats = feats.transpose(1,2)      # Adapt input for conv\n",
    "        # bSz, fSz, ntiles\n",
    "        feats = self.conv1x1(feats)[:,0]  # Feature embedding\n",
    "        # bSz, ntiles\n",
    "\n",
    "        # min-max selection\n",
    "        vals, inds = feats.sort(dim=1)\n",
    "        minmax_inds = torch.cat([inds[:,:self.R] , inds[:,-self.R:]], dim=1)\n",
    "        minmax_feats = torch.gather(feats, dim=1, index=minmax_inds)\n",
    "\n",
    "        logits = self.mlp(minmax_feats)\n",
    "        # bSz, 1\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        out = {\n",
    "            'logits':logits, 'probs':probs,\n",
    "        }\n",
    "        if targets is not None:\n",
    "            # compute loss\n",
    "            loss = self.BCELoss(logits, targets[:,None].float())\n",
    "            loss += 0.5 * (self.conv1x1.weight**2).sum() # Add weight decay\n",
    "            out['loss'] = loss\n",
    "            \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset()\n",
    "model_ = Model()\n",
    "\n",
    "\n",
    "# # TODO: data augmentation ?\n",
    "\n",
    "def fit_and_score(train_dset, val_dset=None, run=0):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dset, batch_size=10, shuffle=True, drop_last=True)\n",
    "    if val_dset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(val_dset, batch_size=10, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = copy.deepcopy(model_).cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train loop\n",
    "    nepochs = 30\n",
    "\n",
    "    def validate():\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for feats, targets in val_loader:\n",
    "            all_targets.append(targets.numpy())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(feats.cuda())\n",
    "            all_preds.append(out['probs'].cpu().numpy())\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        auc = sklearn.metrics.roc_auc_score(all_targets, all_preds)\n",
    "\n",
    "        return auc\n",
    "\n",
    "    writer = SummaryWriter(f'runs/run_{run}')\n",
    "\n",
    "    iteration = 0\n",
    "    best_auc = 0\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        for feats, targets in train_loader:\n",
    "            feats = feats.cuda()\n",
    "            targets = targets.cuda()\n",
    "            out = model(feats, targets=targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out['loss'].backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Train/loss\", out['loss'], iteration)\n",
    "            iteration += 1\n",
    "\n",
    "        if val_dset is None:\n",
    "            continue\n",
    "\n",
    "        # validate and keep the best at each epoch\n",
    "        auc = validate()\n",
    "        writer.add_scalar(\"Val/AUC\", auc, iteration)\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            # save best model\n",
    "            ckpt = {\n",
    "                'model':model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch':epoch,\n",
    "                'auc':auc,\n",
    "            }\n",
    "            torch.save(ckpt, os.path.join(writer.logdir, 'model_best.pth'))\n",
    "        # print(f\"Epoch {epoch}: AUC {auc:0.2f}, best AUC {best_auc:0.2f}\")\n",
    "\n",
    "    if val_dset is None:\n",
    "        ckpt = {\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch':epoch,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(writer.logdir, 'model_final.pth'))\n",
    "\n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "seed: 0 - CV [0/5] -  AUC: 0.90\n",
      "seed: 0 - CV [1/5] -  AUC: 0.81\n",
      "seed: 0 - CV [2/5] -  AUC: 0.55\n",
      "seed: 0 - CV [3/5] -  AUC: 0.77\n",
      "seed: 0 - CV [4/5] -  AUC: 0.71\n",
      "seed: 1 - CV [0/5] -  AUC: 0.62\n",
      "seed: 1 - CV [1/5] -  AUC: 0.91\n",
      "seed: 1 - CV [2/5] -  AUC: 0.64\n",
      "seed: 1 - CV [3/5] -  AUC: 0.62\n",
      "seed: 1 - CV [4/5] -  AUC: 0.78\n",
      "seed: 2 - CV [0/5] -  AUC: 0.70\n",
      "seed: 2 - CV [1/5] -  AUC: 0.62\n",
      "seed: 2 - CV [2/5] -  AUC: 0.88\n",
      "seed: 2 - CV [3/5] -  AUC: 0.80\n",
      "seed: 2 - CV [4/5] -  AUC: 0.75\n",
      "seed: 3 - CV [0/5] -  AUC: 0.65\n",
      "seed: 3 - CV [1/5] -  AUC: 0.62\n",
      "seed: 3 - CV [2/5] -  AUC: 0.89\n",
      "seed: 3 - CV [3/5] -  AUC: 0.91\n",
      "seed: 3 - CV [4/5] -  AUC: 0.66\n",
      "seed: 4 - CV [0/5] -  AUC: 0.65\n",
      "seed: 4 - CV [1/5] -  AUC: 0.68\n",
      "seed: 4 - CV [2/5] -  AUC: 0.89\n",
      "seed: 4 - CV [3/5] -  AUC: 0.64\n",
      "seed: 4 - CV [4/5] -  AUC: 0.71\n",
      "seed: 5 - CV [0/5] -  AUC: 0.68\n",
      "seed: 5 - CV [1/5] -  AUC: 0.78\n",
      "seed: 5 - CV [2/5] -  AUC: 0.75\n",
      "seed: 5 - CV [3/5] -  AUC: 0.69\n",
      "seed: 5 - CV [4/5] -  AUC: 0.64\n",
      "seed: 6 - CV [0/5] -  AUC: 0.60\n",
      "seed: 6 - CV [1/5] -  AUC: 0.64\n",
      "seed: 6 - CV [2/5] -  AUC: 0.70\n",
      "seed: 6 - CV [3/5] -  AUC: 0.73\n",
      "seed: 6 - CV [4/5] -  AUC: 0.58\n",
      "seed: 7 - CV [0/5] -  AUC: 0.57\n",
      "seed: 7 - CV [1/5] -  AUC: 0.63\n",
      "seed: 7 - CV [2/5] -  AUC: 0.65\n",
      "seed: 7 - CV [3/5] -  AUC: 0.64\n",
      "seed: 7 - CV [4/5] -  AUC: 0.69\n",
      "seed: 8 - CV [0/5] -  AUC: 0.76\n",
      "seed: 8 - CV [1/5] -  AUC: 0.69\n",
      "seed: 8 - CV [2/5] -  AUC: 0.67\n",
      "seed: 8 - CV [3/5] -  AUC: 0.64\n",
      "seed: 8 - CV [4/5] -  AUC: 0.53\n",
      "seed: 9 - CV [0/5] -  AUC: 0.65\n",
      "seed: 9 - CV [1/5] -  AUC: 0.71\n",
      "seed: 9 - CV [2/5] -  AUC: 0.88\n",
      "seed: 9 - CV [3/5] -  AUC: 0.77\n",
      "seed: 9 - CV [4/5] -  AUC: 0.87\n",
      "Predicting weak labels using Chowder\n",
      "AUC: mean 0.7100294329014395, std 0.09980824066332905\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_runs = 10\n",
    "num_splits = 5\n",
    "\n",
    "aucs = []\n",
    "for seed in range(num_runs):\n",
    "    # create new model\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True,random_state=seed)\n",
    "\n",
    "    targets = dset.training_output['Target'].tolist()\n",
    "\n",
    "    cv_aucs = []\n",
    "    for i, (train_inds, val_inds) in enumerate(cv.split(dset, y=targets)):\n",
    "        train_dset = torch.utils.data.Subset(dset, train_inds)\n",
    "        val_dset = torch.utils.data.Subset(dset, val_inds)\n",
    "\n",
    "        # train model on train_dset; evaluate on val_dset\n",
    "        auc = fit_and_score(train_dset, val_dset, run=f'seed_{seed}_cv_{i}')\n",
    "        print(f\"seed: {seed} - CV [{i}/{num_splits}] -  AUC: {auc:0.2f}\")\n",
    "        cv_aucs.append(auc)\n",
    "    \n",
    "    # cv_auc = np.array(cv_aucs).mean()\n",
    "    aucs.append(cv_aucs)\n",
    "\n",
    "aucs = np.array(aucs)\n",
    "\n",
    "print(\"Predicting weak labels using Chowder\")\n",
    "print(\"AUC: mean {}, std {}\".format(aucs.mean(), aucs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training final model\n",
      "AUC for model trained on all data 0\n",
      "Test dataset has 120\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "# generate the submission file\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Prediction on the test set\n",
    "\n",
    "# Train a final model on the full training set\n",
    "train_dset = dset\n",
    "label = 'final'\n",
    "print(\"Training final model\")\n",
    "_ = fit_and_score(train_dset, val_dset=None, run=label)\n",
    "model = Model()\n",
    "ckpt = torch.load(f'runs/run_{label}/model_final.pth')\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model = model.cuda().eval()\n",
    "\n",
    "test_dset = TestDataset()\n",
    "\n",
    "loader = torch.utils.data.DataLoader(test_dset, batch_size=len(test_dset), shuffle=False)\n",
    "\n",
    "feats, IDs = iter(loader).next()\n",
    "\n",
    "# load test features\n",
    "with torch.no_grad():\n",
    "    preds_test = model(feats.cuda())['probs'].cpu().numpy()[:,0]\n",
    "\n",
    "print(preds_test.shape)\n",
    "\n",
    "# Check that predictions are in [0, 1]\n",
    "assert np.max(preds_test) <= 1.0\n",
    "assert np.min(preds_test) >= 0.0\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Write the predictions in a csv file, to export them in the suitable format\n",
    "# to the data challenge platform\n",
    "test_output = pd.DataFrame({\"ID\": IDs, \"Target\": preds_test})\n",
    "test_output.set_index(\"ID\", inplace=True)\n",
    "test_output.to_csv(data_dir / \"preds_test_chowder.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the annotated tiles from data/train_input/train_tile_annotation.csv to compare tumoral and non tumoral tiles."
   ]
  }
 ]
}